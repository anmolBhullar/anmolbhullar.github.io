\documentclass[a4paper, 11pt]{book}

\newcommand*{\plogo}{\fbox{$\mathcal{DB}$}}

\usepackage[utf8]{inputenc}
\usepackage{stix}
\usepackage[english]{babel}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{nicefrac}
\usepackage{commath}
\usepackage{dirtytalk}

\usepackage{graphicx}

\theoremstyle{definition}

\newtheorem{theorem}{Theorem}
\newtheorem{es}{Examples}
\newtheorem*{example}{Example}

\newcommand\restr[2]{{% we make the whole thing an ordinary symbol
    \left.\kern-\nulldelimiterspace % automatically resize the bar with \right
    #1 % the function
    \vphantom{\big|} % pretend it's a little taller at normal size
    \right|_{#2} % this is the delimiter
}}

\newtheorem{definition}{Definition}[section]
 
\theoremstyle{remark}
\newtheorem*{remark}{Remark}

\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}[theorem]{Lemma}

\newtheorem{prop}{Proposition}

\newcommand{\inter}[1]{int(#1)}

\begin{document}

    \begin{titlepage} 
        \raggedleft

        \rule{1pt}{\textheight} % Vertical line
        \hspace{0.05\textwidth} % Whitespace between the vertical line and title page text
        \parbox[b]{0.75\textwidth}{ % Paragraph box for holding the title page text, adjust the width to move the title page 
            % left or right on the page
                            
            {\Huge\bfseries MATD11 Lecture \\[0.5\baselineskip] Notes }\\[2\baselineskip] % Title
            {\large\textit{Lecture notes on a course in elemntary functional analysis}}\\[4\baselineskip] % Subtitle
            {\Large\textsc{A. Wortschöpfer}} % Author name, lower case for consistent small caps
                                                    
            \vspace{0.5\textheight} % Whitespace between the title block and the publisher
                                                            
            {\noindent The Publisher~~\plogo}\\[\baselineskip] % Publisher and logo
        }
    \end{titlepage}

    \chapter{Hilbert Space Preliminaries}

    We start by giving a familiar algebraic object (vector spaces) some additional structure so that we can start using our
    familiar analysis techniques on them.

    For the sake of completeness, we start by the definition of a vector space:
    \begin{definition}[Vector Space]
        A \textbf{vector space} over a field (in this course, we will either just use $\mathbb{R}$ or $\mathbb{C}$) $F$ is a set
        $V$ together with two operations that satisfy the eight axioms below:
        \begin{itemize}
            \item The first operation, called \textbf{vector addition} is a function $+: V \times V \to V$ which takes any two
                \textit{vectors} (elements of $V$) and assigns them to another vector of $V$, usually denoted by $v+w$.
            \item The second operation, called \textbf{scalar multiplication} is a function $\cdot: F\times V\to V$, takes any
                \textit{scalar} (elements of $F$) $a$ of $F$ and any vector $v$ of $V$ and assigns it to another vector of $V$, usually
                denoted by $av$.
        \end{itemize}
        The eight axioms are as follows: For any $u,v,w\in V$ and $a,b\in F$:
        \begin{enumerate}
            \item Associativity of addition: $u + (v + w) = (u + v) + w$
            \item Commutativity of addition: $u + v = v + w$
            \item Identity elements of addition: There exists an element denoted $0$ of $V$ such that $v+0 = v$ for all $v\in V$
            \item Inverse elements of addition: For every $v\in V$, there exists an element denoted $-v$ of $V$ such that $v + (-v) = 0$
            \item Compatibility of scalar multiplication with field multiplication: $a(bv) = (ab)v$
            \item Identity element of scalar multiplication: $1v = v$ where $1$ is the multiplicative identity of $F$
            \item Distributivity of scalar multiplication wrt vector addition: $a(v+w) = av+aw$
            \item Distributivity of scalar multiplication wrt field addition: $(a+b)v = av + bv$
        \end{enumerate}
    \end{definition}

    To introduce notions such as convergence or more generally, limits, we introduce the structure of a \textit{norm} over a vector
    space.

    \begin{definition}[Normed Linear Space]
        Let $X$ be a vector space over either the scalar field $\mathbb{R}$ of real numbers or the scalar field $\mathbb{C}$ of
        complex numbers. Suppose we have a function $\norm{\cdot}: X \to [0,\infty)$ such that
        \begin{enumerate}
            \item $\norm{x} = 0$ if and only if $x = 0$,
            \item $\norm{x+y} \leq \norm{x} + \norm{y}$ for all $x,y\in X$, and
            \item $\norm{\alpha x} = |\alpha|\norm{x}$ for all scalars $\alpha$ and vectors $x$.
        \end{enumerate}
        We call $(X,\norm{\cdot})$ a \textbf{normed linear space}. Property (2) is called \textbf{triangle inequality}, and property
        (3) is called \textbf{homogeneity}.
    \end{definition}

    Before diving into these special structures, we recall the definition of a metric space.

    \begin{definition}
        A \textbf{metric space} is a set $X$ with a function $d(\cdot,\cdot):X\times X\to [0,\infty)$ satisfying, for $x,y$ and
        $z$ in $X$,
        \begin{enumerate}
            \item $d(x,y) = 0$ if and only if $x=y$,
            \item $d(x,y) = d(y,x)$, and
            \item $d(x,y) + d(y,z) \geq d(x,z)$.
        \end{enumerate}
        The third property is referred to as the triangle inequality.
    \end{definition}

    \begin{remark}
        Every norm induces a metric. Let $(X,\norm{\cdot})$ be a normed linear space (now abbreviated \textit{nvs}), and define
        $d: X\times X \to [0,\infty)$ by $d(x,y) = \norm{x-y}$. After, checking that $(X,d)$ is indeed a metric space, one finds
        that a normed linear space induces a metric space $(X,d)$.
    \end{remark}

    \begin{remark}
        Note, if we have a metric space, we can quickly deduce the norm by setting $\norm{x} = d(0,x)$. Therefore, we can interpret
        $\norm{x}$ to be the distance from 0 to $x$.
    \end{remark}

    \begin{remark}
        The metric induced from a norm is \textbf{translation invariant} i.e.
        \[ \forall\;x,y,z\in X,\:\text{we have}\; d(x+z,y+z) = d(x,y) \]
    \end{remark}

    \begin{remark}
        In a nvs, $(X,\norm{\cdot})$, we have,
        \[ \norm{0} = \norm{ 0 \cdot 0} = 0\norm{0} = 0 \]
        Therefore, the first property of a normed vector space can be shortened to:
        \[ \forall\; x\in X,\:\text{if}\:|x|=0\implies x=0 \]
    \end{remark}

    Through this induced metric, we have a topology which allows us to talk about limits and completeness. We abuse this and
    give the following definitions:

    \begin{definition}
        Let $X$ be a metric space. A sequence $\{x_n\}$ in $X$ is said to be a \textbf{Cauchy sequence} if it has the following
        property: Given any $\epsilon>0$ there exists $N$ such that if $n,m\geq N$, then $d(x_n,x_m)<\epsilon$.
    \end{definition}

    \begin{definition}
        A metric space is said to be \textbf{complete} if every Cauchy sequence in $X$ converges in $X$ (i.e. converges to a point
        in $X$).
    \end{definition}

    \begin{definition}
        Let $X$ be a normed linear space. If $X$ is complete in the metric $d$ defined from the norm by $d(x,y)=\norm{x-y}$, we
        call $X$ a \textbf{Banach space}.
    \end{definition}

    We give some examples of normed linear spaces, some of which are Banach spaces.

    \begin{example}
        Consider $(\mathbb{R}^n,\norm{\cdot})$ where $\norm{\cdot}$ is given by $\norm{x} = \sum_{i=1}^n x_i^2$ where 
        $x = (x_1,\hdots,x_n)$ and $x_i\in\mathbb{R}$. The triangle inequality in this case, for $\mathbb{R}^n$ is proved
        using the Cauchy-Schwarz Inequality which states that $|x\cdot y| \leq \norm{x}\norm{y}$ where $x\cdot y = \sum x_iy_i$.
        Thus, $(\mathbb{R}^n,\norm{\cdot})$ is a normed linear space.\\

        We claim that this space is also Banach.\\

        Let $\{x_j\}_{j=1}^{\infty}$ be a Cauchy sequence in $\mathbb{R}^n$ so that each $x_j\in\mathbb{R}^n$. We write,
        $x_j = (x_j(1),\hdots,x_j(n))\in\mathbb{R}^n$. Since, we have a Cauchy sequence, given $\epsilon>0$, there exists
        $N\in\mathbb{N}$ such that if $m,n\geq N$, then $d(x_m,x_n)<\epsilon$. This implies,
        \[ \sum_{i=1}^n |x_m(i) - x_n(i)|^2 < \epsilon^2 \qquad\text{when}\;m,n\geq N \]
        Since each term in the sum is at least 0, we obtain that for all $i\in\{1,\hdots,n\}$, 
        if $m,n\geq N$, then $|x_m(i)-x_n(i)| < \epsilon$. This further implies that for all $i\in\{1,\hdots,n\}$, the sequence
        $\{x_j(i)\}_{j=1}^{\infty}$ is a Cauchy sequence (in $\mathbb{R}$).\\
        By the Cauchy Criterion (Cauchy sequences are convergent), the Cauchy sequence converges so that we can write,
        $x(i) = \lim_{j\to\infty} x_j(i)$. We let,
        \[ \mathbf{x} = (x_1,x_2,\hdots,x_n)\in\mathbb{R}^n \]
        We check that $\lim_{j\to\infty} x_j = x$. Therefore, $\{x_j\}_{j=1}^{\infty}$ converges (in $\mathbb{R}$), hence
        $\mathbb{R}^n$ is complete.
    \end{example}

    \begin{example}
        Note, $\mathbb{C}^n$ is also a Banach space. Define for any $z\in\mathbb{C}^n$,
        \[ \norm{z} = \sqrt{\sum_{i=1}^{\infty} |z_i|^2} \]
    \end{example}

    \begin{example}
        Let $c_0(\mathbb{R}) = \{\text{real sequences that "vanishes at $\infty$"}\}$ i.e.,
        \[  \{\alpha=\{\alpha_n\}_{n=1}^{\infty}: \alpha_n\in\mathbb{R},\:\lim_{n\to\infty} \alpha_n = 0\} \]
        Here, the vector space operations are component-wise. Specifically, if $\alpha = \{\alpha_n\}_{n=1}^{\infty},
        \beta = \{\beta\}_{n=1}^{\infty} \in c_0(\mathbb{R})$, then
        \[ \alpha + \beta = \{\alpha_n+\beta_n\}_{n=1}^{\infty} \]
        and if $c\in\mathbb{R}$, then
        \[ c\alpha = \{c\alpha_n\}_{n=1}^{\infty} \]
        Define $\norm{\alpha}_{\infty} = \norm{\{\alpha_n\}}_{\infty} =$ sup$\{|a_n|:n\in\mathbb{N}\}$
        Note that the supremum always exists since convergent sequences are bounded and non-empty. Therefore, $\alpha_n\to0$ so that
        $|\alpha_n|\to0$, also
        \[ \norm{\cdot}_{\infty}: c_0(\mathbb{R}) \to [0,\infty) \]
        We can check the triangle inequality. Let $\alpha,\beta\in c_0(\mathbb{R})$
        \[ \alpha = \{\alpha\}_{n=1}^{\infty}, \quad\alpha_n\to0, \quad\beta = \{\beta_n\}_{n=1}^{\infty}, \quad\beta_n\to0 \]
        Therefore, for all $n\in\mathbb{N}$,
        \[ |\alpha_n + \beta_n| \leq |\alpha_n| + |\beta_n| \leq \norm{\alpha}_{\infty} + \norm{\beta}_{\infty} \]
        Therefore, sup$\{|\alpha_n + \beta_n|: n\in\mathbb{N}\} \leq \norm{\alpha}_{\infty} + \norm{\beta}_{\infty}$ 
        Therefore, $(c_0(\mathbb{R}),\norm{\cdot}_{\infty})$ is a normed linear space and one can check that it is a Banach space.
        \[ \{a_n\}_{n=1}^{\infty},\quad \alpha_n = (\alpha_n(1),\alpha_n(2), \hdots) \]
    \end{example}

    \begin{example}
        For $p\in[1,\infty)$, let 
        \[ \ell^p(\mathbb{R}) = \{ a = \{a_n\}_{n=1}^{\infty}: a_n\in\mathbb{R}, \sum_{n=1}^{\infty} |a_n|^p < \infty\} \]
        where
        \[ \norm{a}_p = (\sum_{n=1}^{\infty} |a_n|^p)^{\nicefrac{1}{p}}\quad\text{where}\; a\in\ell^p(\mathbb{R}) \]
        For $p=\infty$, define
        \[ \ell^p(\mathbb{R}) = \{ a = \{a_n\}_{n=1}^{\infty}: a_n\in\mathbb{R},\:\text{sup}\{|a_n|:n\in\mathbb{N}\} < \infty \} \]
        i.e. $\ell^p(\mathbb{R})$ consists of bounded real sequences. The norm on this is given by,
        \[ \norm{a}_{\infty} =\;\text{sup}\{|a_n|: n\in\mathbb{N}\} \]
        For $1\leq p\leq \infty$, $\norm{a}_p$ is a norm on $\ell^p(\mathbb{R})$. Let's look at the triangle inequality for
        the norm $\norm{\cdot}_p$.\\

        We start with a version of \textit{Hölder's inequality}. If $1\leq p<\infty$ and $q$ is defined by $\frac{1}{p}+\frac{1}{q}=1$
        (here $p$ and $q$ are called \textit{conjugate indices})
    \end{example}

\end{document}
